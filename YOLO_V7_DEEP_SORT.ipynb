{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLCSvrDhbhIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c73bd9-8aa8-40fd-bd96-52d068c4d62c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnMGym7rciqc",
        "outputId": "39f6966d-db3b-4051-9c18-8fe0339e89fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1b71hwMc2irBqih7QEhWaQsH2eMvIUCPY/MiniProject6thSem/yolov7\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/MiniProject6thSem/yolov7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31LAJ9rJeMCT",
        "outputId": "3b369678-2b36-4c38-f526-924ddaebc393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.23.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.9.0.80)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (10.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.11.4)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.2.0+cpu)\n",
            "Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.17.0+cpu)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.66.2)\n",
            "Requirement already satisfied: protobuf<4.21.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.20.3)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.15.2)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (0.13.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (7.34.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (5.9.5)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 36)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (2024.3.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.62.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2024.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (2.17.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (4.9.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 34)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 34)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 34)) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.2.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.10)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.23.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (10.3.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.34.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.4.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (24.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.23.4)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.11)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.23)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.23)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mediapipe) (2.2.0+cpu)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.9.0.80)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.6)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe) (2024.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mediapipe) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.3)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.9.0.80)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.0+cpu)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Collecting easydict\n",
            "  Downloading easydict-1.13-py3-none-any.whl (6.8 kB)\n",
            "Installing collected packages: easydict\n",
            "Successfully installed easydict-1.13\n",
            "Requirement already satisfied: numpy==1.23.4 in /usr/local/lib/python3.10/dist-packages (1.23.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install cython\n",
        "!pip install scikit-image\n",
        "!pip install opencv-python-headless\n",
        "!pip install mediapipe\n",
        "!pip install torch torchvision\n",
        "!pip install ultralytics\n",
        "!pip install easydict\n",
        "!pip install numpy==1.23.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLXggOBUxhlu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp #face detector\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "import torch\n",
        "import torch.nn as  nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Ensure that imports for DeepSort and YOLO are correctly handled\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, non_max_suppression, scale_coords, xyxy2xywh\n",
        "from utils.torch_utils import select_device, time_synchronized\n",
        "from deep_sort_pytorch.utils.parser import get_config\n",
        "from deep_sort_pytorch.deep_sort import DeepSort\n",
        "# Ensure TensorFlow and Keras are correctly installed\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H3Efr98fzIE",
        "outputId": "18c062ac-a362-494a-ff94-9d560066c601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttIW5gqNvFs4"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model\n",
        "model = load_model('./pretrained_models/facialemotionmodel.h5')\n",
        "\n",
        "# Load the cascades for face and eye detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + './pretrained_models/haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Define emotion labels\n",
        "emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
        "\n",
        "# Function to adjust predictions vector based on specified conditions\n",
        "def adjust_predictions(predictions):\n",
        "    # Copy the original predictions vector\n",
        "    predictions = predictions.flatten()\n",
        "    modified_predictions = predictions.copy()\n",
        "    # Remove Disgust and add its probability to Neutral\n",
        "    modified_predictions[4] += modified_predictions[1]\n",
        "    modified_predictions[1] = 0\n",
        "\n",
        "    # Remove Fear and add its probability to Neutral\n",
        "    modified_predictions[4] += modified_predictions[2]\n",
        "    modified_predictions[2] = 0\n",
        "\n",
        "    # Remove Surprise and add its probability to Happy\n",
        "    modified_predictions[3] += modified_predictions[6]\n",
        "    modified_predictions[6] = 0\n",
        "\n",
        "    # Remove Angry and add its probability to Sad\n",
        "    modified_predictions[5] += modified_predictions[0]\n",
        "    modified_predictions[0] = 0\n",
        "    return modified_predictions\n",
        "\n",
        "# Function to detect emotions from face image\n",
        "def predict_emotion(face):\n",
        "    # Resize image to match model input size\n",
        "    face = cv2.resize(face, (48, 48))\n",
        "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "    face = img_to_array(face)  # Convert to array\n",
        "    face = preprocess_input(face)  # Preprocess input for the model\n",
        "    face = np.expand_dims(face, axis=0)  # Add batch dimension\n",
        "    predictions = model.predict(face)\n",
        "    # Adjust predictions\n",
        "    modified_predictions = adjust_predictions(predictions)\n",
        "\n",
        "    # Find the dominant emotion after adjustments\n",
        "    emotion_index = np.argmax(modified_predictions)\n",
        "    emotion = emotions[emotion_index]\n",
        "    return emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AMl_qDHTqQf"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def calculate_distance(keypoint1, keypoint2):\n",
        "    \"\"\"Calculate Euclidean distance between two keypoints.\"\"\"\n",
        "    return math.sqrt((keypoint2[0] - keypoint1[0])**2 + (keypoint2[1] - keypoint1[1])**2)\n",
        "\n",
        "def calculate_angle(keypoint1, keypoint2, reference_vector):\n",
        "    \"\"\"Calculate the angle between two vectors with respect to a reference vector.\"\"\"\n",
        "    vector1 = (keypoint1[0] - keypoint2[0], keypoint1[1] - keypoint2[1])\n",
        "    vector2 = (keypoint2[0] - reference_vector[0], keypoint2[1] - reference_vector[1])\n",
        "\n",
        "    dot_product = vector1[0] * vector2[0] + vector1[1] * vector2[1]\n",
        "    magnitude_product = math.sqrt((vector1[0]**2 + vector1[1]**2) * (vector2[0]**2 + vector2[1]**2))\n",
        "\n",
        "    if magnitude_product == 0:\n",
        "        return None\n",
        "\n",
        "    cosine_angle = dot_product / magnitude_product\n",
        "    angle_rad = math.acos(cosine_angle)\n",
        "    angle_deg = math.degrees(angle_rad)\n",
        "\n",
        "    return angle_deg\n",
        "\n",
        "def get_midpoint(keypoint1, keypoint2):\n",
        "    \"\"\"Calculate the midpoint between two keypoints.\"\"\"\n",
        "    x_mid = (keypoint1[0] + keypoint2[0]) / 2\n",
        "    y_mid = (keypoint1[1] + keypoint2[1]) / 2\n",
        "    return x_mid, y_mid\n",
        "\n",
        "def calculate_distances_and_angles(keypoints):\n",
        "    \"\"\"Calculate distances and angles between specified keypoints.\"\"\"\n",
        "    left_shoulder = keypoints[11]  # Keypoint index 11 is left shoulder\n",
        "\n",
        "    left_elbow = keypoints[13] # Keypoint index 13 is left elbow\n",
        "\n",
        "\n",
        "    left_wrist = keypoints[15] # Keypoint index 15 is left wrist\n",
        "    right_shoulder = keypoints[12] # Keypoint index 12 is right shoulder\n",
        "    right_elbow = keypoints[14]    # Keypoint index 14 is right elbow\n",
        "    right_wrist = keypoints[16]    # Keypoint index 16 is right wrist\n",
        "    nose = keypoints[0]            # Keypoint index 0 is nose\n",
        "    neck = get_midpoint(left_shoulder, right_shoulder)  # Midpoint between left and right shoulders\n",
        "\n",
        "    distances = {\n",
        "        \"left_shoulder_to_left_elbow\": calculate_distance(left_shoulder, left_elbow),\n",
        "        \"left_elbow_to_left_wrist\": calculate_distance(left_elbow, left_wrist),\n",
        "        \"right_shoulder_to_right_elbow\": calculate_distance(right_shoulder, right_elbow),\n",
        "        \"right_elbow_to_right_wrist\": calculate_distance(right_elbow, right_wrist),\n",
        "        \"nose_to_neck\": calculate_distance(nose, neck)\n",
        "    }\n",
        "\n",
        "#     print(neck[0], neck[1] - 1)\n",
        "\n",
        "    # angles = {\n",
        "    #     \"phi1\": calculate_angle(nose, neck, (neck[0], neck[1] - 1)),  # Reference vector: straight up\n",
        "    #     \"phi2\": calculate_angle(right_elbow, right_shoulder, (right_shoulder[0] + 1, right_shoulder[1])),  # Reference vector: pointing right\n",
        "    #     \"phi3\": calculate_angle(right_wrist, right_elbow, (right_elbow[0], right_elbow[1] - 1)),  # Reference vector: straight up\n",
        "    #     \"phi4\": calculate_angle(left_elbow, left_shoulder, (left_shoulder[0] - 1, left_shoulder[1])),  # Reference vector: pointing left\n",
        "    #     \"phi5\": calculate_angle(left_wrist, left_elbow, (left_elbow[0], left_elbow[1] - 1))  # Reference vector: straight up\n",
        "    # }\n",
        "\n",
        "    angles = {\n",
        "        \"phi1\": calculate_angle(nose, neck, (neck[0], neck[1] + 1)),  # Reference vector: perpendicular to the horizontal plane\n",
        "        \"phi2\": calculate_angle(right_elbow, right_shoulder, (right_shoulder[0] + 1, right_shoulder[1])),  # Reference vector: pointing right\n",
        "        \"phi3\": calculate_angle(right_wrist, right_elbow, (right_elbow[0], right_elbow[1] + 1)),  # Reference vector: horizontal\n",
        "        \"phi4\": calculate_angle(left_elbow, left_shoulder, (left_shoulder[0] - 1, left_shoulder[1])),  # Reference vector: pointing left\n",
        "        \"phi5\": calculate_angle(left_wrist, left_elbow, (left_elbow[0], left_elbow[1] + 1))  # Reference vector: horizontal\n",
        "    }\n",
        "\n",
        "\n",
        "    return distances, angles\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjVu700Mj6ob",
        "outputId": "bacb619f-fa94-4c64-bfb0-e63baf0dd080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbk5In_mWsdv"
      },
      "outputs": [],
      "source": [
        "def classify_learning_emotion(emotion, action):\n",
        "    positive_emotions = ['Neutral', 'Happy', 'Surprise']\n",
        "    negative_emotions = ['Sad', 'Fear', 'Disgust', 'Angry']\n",
        "    attentive_postures = ['Looking_Forward', 'Raising_Hand', 'Reading', 'Writting']\n",
        "    inattentive_postures = ['Sleeping', 'Turning_Around']\n",
        "    if emotion in positive_emotions and action in attentive_postures:\n",
        "        return \"Focused\"\n",
        "    elif emotion in negative_emotions and action in attentive_postures:\n",
        "        return \"Confused\"\n",
        "    elif emotion in positive_emotions and action in inattentive_postures:\n",
        "        return \"Distracted\"\n",
        "    elif emotion in negative_emotions and action in inattentive_postures:\n",
        "        return \"Disengaged\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose\n",
        "labels = ['Looking_Forward', 'Raising_Hand', 'Reading', 'Sleeping', 'Turning_Around', 'Writting']\n",
        "model_path = '/content/drive/MyDrive/Mini project 1 /pose action/csv of keypoints remove norm/model_remove_norm_1st.h5'  # Update with the path to your model\n",
        "# model_path = '/content/drive/MyDrive/Mini project 1 /pose action/csv of modified dataset/model_40_fetarue_adam_0.001.h5'\n",
        "action_model = load_model(model_path)\n",
        "\n",
        "\n",
        "def setup_deepsort_and_yolo(weights_path, config_path, img_size=640):\n",
        "    device = select_device(device='cpu' if not torch.cuda.is_available() else 'cuda')\n",
        "\n",
        "    model = attempt_load(weights_path, map_location=device)\n",
        " #\n",
        "    stride = int(model.stride.max())\n",
        "    img_size = check_img_size(img_size, s=stride)\n",
        "    cfg_deep = get_config()\n",
        "    cfg_deep.merge_from_file(config_path)\n",
        "    deepsort = DeepSort(cfg_deep.DEEPSORT.REID_CKPT, max_dist=cfg_deep.DEEPSORT.MAX_DIST,\n",
        "                        min_confidence=cfg_deep.DEEPSORT.MIN_CONFIDENCE, nms_max_overlap=cfg_deep.DEEPSORT.NMS_MAX_OVERLAP,\n",
        "                        max_iou_distance=cfg_deep.DEEPSORT.MAX_IOU_DISTANCE, max_age=cfg_deep.DEEPSORT.MAX_AGE,\n",
        "                        n_init=cfg_deep.DEEPSORT.N_INIT, nn_budget=cfg_deep.DEEPSORT.NN_BUDGET, use_cuda=torch.cuda.is_available())\n",
        "    print(\"Models loaded successfully\")\n",
        "    return model, deepsort, device, img_size\n",
        "\n",
        "def process_video(input_video, output_video, weights_path, config_path):\n",
        "    print(\"Initializing YOLO and DeepSort...\")\n",
        "    model, deepsort, device, img_size = setup_deepsort_and_yolo(weights_path, config_path)\n",
        "    print(\"Processing Video...\")\n",
        "    vid_cap = cv2.VideoCapture(input_video)\n",
        "    vid_writer = None\n",
        "    frame_idx = 0\n",
        "    # fourcc = int(vid_cap.get(cv2.CAP_PROP_FOURCC))\n",
        "    # fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "    while True:\n",
        "        ret, frame = vid_cap.read()\n",
        "        if not ret:\n",
        "            print(\"No frames grabbed!\")\n",
        "            break\n",
        "        frame_idx += 1\n",
        "        print(f\"Processing frame {frame_idx}...\")\n",
        "        # Resize frame while preserving the number of color channels and convert to RGB\n",
        "        frame_resized = cv2.resize(frame, (img_size, img_size))\n",
        "        # Convert frame to tensor with correct shape and number of channels\n",
        "        frame_tensor = torch.from_numpy(frame_resized).to(device).permute(2, 0, 1).float().div(255.0).unsqueeze(0)\n",
        "        with torch.no_grad():  # Add torch.no_grad() to prevent gradient computation\n",
        "            pred = model(frame_tensor, augment=False)[0]\n",
        "            pred = non_max_suppression(pred, 0.4, 0.5, classes=[0], agnostic=False)\n",
        "        if len(pred):\n",
        "            det = pred[0]\n",
        "            if len(det):\n",
        "                det[:, :4] = scale_coords(frame_tensor.shape[2:], det[:, :4], frame_resized.shape).round()\n",
        "                xywhs = xyxy2xywh(det[:, :4])\n",
        "                confs = det[:, 4]\n",
        "                outputs = deepsort.update(xywhs.cpu(), confs.cpu(), frame_resized)\n",
        "                if len(outputs):\n",
        "                    for output in outputs:\n",
        "                        bboxes = output[:4]\n",
        "                        id = output[4]\n",
        "                        x1, y1, x2, y2 = map(int, bboxes)\n",
        "                        body_roi = frame_resized[y1:y2, x1:x2]\n",
        "                        if body_roi.size > 0:\n",
        "                            #detect_emotion(face_roi)\n",
        "                            emotion = predict_emotion(body_roi)\n",
        "                            pose = mp_pose.Pose(static_image_mode=False,\n",
        "                                    model_complexity=2,\n",
        "                                    smooth_landmarks=True,\n",
        "                                    min_detection_confidence=0.5,\n",
        "                                    min_tracking_confidence=0.5)\n",
        "\n",
        "                            results = pose.process(cv2.cvtColor(body_roi, cv2.COLOR_BGR2RGB))\n",
        "                            width = abs(x2-x1)\n",
        "                            height = abs(y2-y1)\n",
        "                            if results.pose_landmarks is None:\n",
        "                                continue\n",
        "                            # if results.pose_landmarks is not None:\n",
        "                            #     mp_drawing.draw_landmarks(frame[y1:y2, x1:x2], results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "\n",
        "                            # Extract keypoints and calculate features\n",
        "                            keypoints = results.pose_landmarks.landmark\n",
        "                            flattened_keypoints = [(kp.x, kp.y) for kp in keypoints[:25]]\n",
        "                            distances, angles = calculate_distances_and_angles(flattened_keypoints)\n",
        "                            sepearted_keypoints = [coord for kp in flattened_keypoints for coord in kp]\n",
        "                            flattened_distances = list(distances.values())\n",
        "                            flattened_angles = list(angles.values())\n",
        "                            pose.close()\n",
        "\n",
        "\n",
        "\n",
        "                            data_row = sepearted_keypoints + flattened_distances + flattened_angles\n",
        "\n",
        "                            data_row = np.array(data_row).reshape(1, -1)\n",
        "                            # Predict action\n",
        "                            predicted_action = action_model.predict(data_row)\n",
        "                            predicted_index = np.argmax(predicted_action[0])\n",
        "                            predicted_label = labels[predicted_index]\n",
        "                            # cv2.rectangle(frame_resized, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                            # cv2.putText(frame_resized, f'ID {id}: {emotion} Action:{labels[predicted_index]}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "                            learning_emotion = classify_learning_emotion(emotion,predicted_label)\n",
        "                            overlay_text = f'ID {id}:{learning_emotion}'\n",
        "                            print(f'Id :{id } :emotion:{emotion} action {predicted_label} le {learning_emotion}')\n",
        "\n",
        "                            # Draw bounding box and overlay text\n",
        "                            cv2.rectangle(frame_resized, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                            text_size = cv2.getTextSize(overlay_text, cv2.FONT_HERSHEY_SIMPLEX, 0.9, 2)[0]\n",
        "                            text_x = max(0, x1 - 5)\n",
        "                            text_y = max(0, y1 - 5)\n",
        "                            cv2.putText(frame_resized, overlay_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "# #idhar apna code add kar de\n",
        "#                             cv2.rectangle(frame_resized, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "#                             cv2.putText(frame_resized, f'ID {id}: {emotion}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "        if vid_writer is None and output_video:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "            vid_writer = cv2.VideoWriter(output_video, fourcc, 30, (frame_resized.shape[1], frame_resized.shape[0]))\n",
        "            print(\"Video writer initialized\")\n",
        "        if vid_writer is not None:\n",
        "            vid_writer.write(frame_resized)\n",
        "    vid_cap.release()\n",
        "    if vid_writer:\n",
        "        vid_writer.release()\n",
        "        print(\"Video writer released\")\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"Done processing video\")\n",
        "# Example usage\n",
        "process_video('./input_data/tanmay_harsh_cut.mp4', './output_data/tanmay_harsh_cut_Ouput.mp4', './pretrained_models/yolov7_training.pt', './deep_sort_pytorch/configs/deep_sort.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Below this is the code for posture and the the integrated code**"
      ],
      "metadata": {
        "id": "nkjokRS5xhJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDUyb5jVSX7N"
      },
      "outputs": [],
      "source": [
        "##integrated\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def classify_learning_emotion(emotion, action):\n",
        "    positive_emotions = ['Neutral', 'Happiness', 'Surprise']\n",
        "    negative_emotions = ['Sadness', 'Fear', 'Disgust', 'Anger']\n",
        "    attentive_postures = ['Looking_Forward', 'Raising_Hand', 'Reading', 'Writting']\n",
        "    inattentive_postures = ['Sleeping', 'Turning_Around']\n",
        "    if emotion in positive_emotions and action in attentive_postures:\n",
        "        return \"Focused\"\n",
        "    elif emotion in negative_emotions and action in attentive_postures:\n",
        "        return \"Confused\"\n",
        "    elif emotion in positive_emotions and action in inattentive_postures:\n",
        "        return \"Distracted\"\n",
        "    elif emotion in negative_emotions and action in inattentive_postures:\n",
        "        return \"Disengaged\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose\n",
        "labels = ['Looking_Forward', 'Raising_Hand', 'Reading', 'Sleeping', 'Turning_Around', 'Writting']\n",
        "model_path = '/content/drive/MyDrive/Mini project 1 /pose action/csv of keypoints remove norm/model_remove_norm_1st.h5'  # Update with the path to your model\n",
        "# model_path = '/content/drive/MyDrive/Mini project 1 /pose action/csv of modified dataset/model_40_fetarue_adam_0.001.h5'\n",
        "action_model = load_model(model_path)\n",
        "\n",
        "def setup_deepsort_and_yolo(weights_path, config_path, img_size=640):\n",
        "    device = select_device(device='cpu' if not torch.cuda.is_available() else 'cuda')\n",
        "    model = attempt_load(weights_path, map_location=device)\n",
        "    stride = int(model.stride.max())\n",
        "    img_size = check_img_size(img_size, s=stride)\n",
        "    cfg_deep = get_config()\n",
        "    cfg_deep.merge_from_file(config_path)\n",
        "    deepsort = DeepSort(cfg_deep.DEEPSORT.REID_CKPT, max_dist=cfg_deep.DEEPSORT.MAX_DIST,\n",
        "                        min_confidence=cfg_deep.DEEPSORT.MIN_CONFIDENCE, nms_max_overlap=cfg_deep.DEEPSORT.NMS_MAX_OVERLAP,\n",
        "                        max_iou_distance=cfg_deep.DEEPSORT.MAX_IOU_DISTANCE, max_age=cfg_deep.DEEPSORT.MAX_AGE,\n",
        "                        n_init=cfg_deep.DEEPSORT.N_INIT, nn_budget=cfg_deep.DEEPSORT.NN_BUDGET, use_cuda=torch.cuda.is_available())\n",
        "    print(\"Models loaded successfully\")\n",
        "    return model, deepsort, device, img_size\n",
        "\n",
        "def process_video(input_video, output_video, weights_path, config_path):\n",
        "    print(\"Initializing YOLO and DeepSort...\")\n",
        "    model, deepsort, device, img_size = setup_deepsort_and_yolo(weights_path, config_path)\n",
        "    print(\"Processing Video...\")\n",
        "    vid_cap = cv2.VideoCapture(input_video)\n",
        "    vid_writer = None\n",
        "    frame_idx = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = vid_cap.read()\n",
        "        if not ret:\n",
        "            print(\"No frames grabbed!\")\n",
        "            break\n",
        "        frame_idx += 1\n",
        "        print(f\"Processing frame {frame_idx}...\")\n",
        "\n",
        "        # Convert frame to tensor with correct shape and number of channels\n",
        "        frame_tensor = torch.from_numpy(frame).to(device).permute(2, 0, 1).float().div(255.0).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(frame_tensor, augment=False)[0]\n",
        "            pred = non_max_suppression(pred, 0.4, 0.5, classes=[0], agnostic=False)\n",
        "\n",
        "        if len(pred):\n",
        "            det = pred[0]\n",
        "            if len(det):\n",
        "                det[:, :4] = scale_coords(frame_tensor.shape[2:], det[:, :4], frame.shape).round()\n",
        "                xywhs = xyxy2xywh(det[:, :4])\n",
        "                confs = det[:, 4]\n",
        "                outputs = deepsort.update(xywhs.cpu(), confs.cpu(), frame)\n",
        "\n",
        "                if len(outputs):\n",
        "                    for output in outputs:\n",
        "                        bboxes = output[:4]\n",
        "                        id = output[4]\n",
        "                        x1, y1, x2, y2 = map(int, bboxes)\n",
        "                        body_roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "                        if body_roi.size > 0:\n",
        "                            emotion = predict_emotion(body_roi)\n",
        "\n",
        "                            pose = mp_pose.Pose(static_image_mode=False,\n",
        "                                                model_complexity=2,\n",
        "                                                smooth_landmarks=True,\n",
        "                                                min_detection_confidence=0.5,\n",
        "                                                min_tracking_confidence=0.5)\n",
        "\n",
        "                            results = pose.process(cv2.cvtColor(body_roi, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "                            if results.pose_landmarks is not None:\n",
        "                                keypoints = results.pose_landmarks.landmark\n",
        "                                flattened_keypoints = [(kp.x, kp.y) for kp in keypoints[:25]]\n",
        "                                distances, angles = calculate_distances_and_angles(flattened_keypoints)\n",
        "                                sepearted_keypoints = [coord for kp in flattened_keypoints for coord in kp]\n",
        "                                flattened_distances = list(distances.values())\n",
        "                                flattened_angles = list(angles.values())\n",
        "                                pose.close()\n",
        "\n",
        "                                data_row = sepearted_keypoints + flattened_distances + flattened_angles\n",
        "                                data_row = np.array(data_row).reshape(1, -1)\n",
        "\n",
        "                                predicted_action = action_model.predict(data_row)\n",
        "                                predicted_index = np.argmax(predicted_action[0])\n",
        "                                predicted_label = labels[predicted_index]\n",
        "\n",
        "                                learning_emotion = classify_learning_emotion(emotion, predicted_label)\n",
        "                                overlay_text = f'ID {id}:{learning_emotion}'\n",
        "                                print(f'Id :{id } :emotion:{emotion} action {predicted_label} le {learning_emotion}')\n",
        "\n",
        "                                # Draw bounding box and overlay text on the original frame\n",
        "                                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                                text_size = cv2.getTextSize(overlay_text, cv2.FONT_HERSHEY_SIMPLEX, 0.9, 2)[0]\n",
        "                                text_x = max(0, x1 - 5)\n",
        "                                text_y = max(0, y1 - 5)\n",
        "                                cv2.putText(frame, overlay_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "        if vid_writer is None and output_video:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "            vid_writer = cv2.VideoWriter(output_video, fourcc, 30, (frame.shape[1], frame.shape[0]))\n",
        "            print(\"Video writer initialized\")\n",
        "\n",
        "        if vid_writer is not None:\n",
        "            vid_writer.write(frame)\n",
        "\n",
        "    vid_cap.release()\n",
        "\n",
        "    if vid_writer:\n",
        "        vid_writer.release()\n",
        "        print(\"Video writer released\")\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"Done processing video\")\n",
        "\n",
        "# Example usage\n",
        "process_video('./input_data/test.mp4', './output_data/op1_combine2.mp4', './pretrained_models/yolov7_training.pt', './deep_sort_pytorch/configs/deep_sort.yaml')\n",
        "\n",
        "# process_video('./input_data/3people.mp4', './output_data/output1.mp4', './pretrained_model/yolov7_training.pt', './deep_sort_pytorch/configs/deep_sort.yaml')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}